{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import inflection\n",
    "import os.path\n",
    "import base64\n",
    "import yaml\n",
    "from sodapy import Socrata\n",
    "import itertools\n",
    "import numpy as np\n",
    "import smtplib\n",
    "from email.MIMEMultipart import MIMEMultipart\n",
    "from email.MIMEText import MIMEText\n",
    "from email.MIMEBase import MIMEBase\n",
    "from email import encoders\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "from retry import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConfigItems:\n",
    "    '''\n",
    "    util class to grab config params from a .yaml file\n",
    "    '''\n",
    "    def __init__(self, inputdir, fieldConfigFile):\n",
    "        self.inputdir = inputdir\n",
    "        self.fieldConfigFile = fieldConfigFile\n",
    "        \n",
    "    def getConfigs(self):\n",
    "        configItems = 0\n",
    "        with open(self.inputdir + self.fieldConfigFile ,  'r') as stream:\n",
    "            try:\n",
    "                configItems = yaml.load(stream)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "        return configItems\n",
    "\n",
    "class SocrataClient:\n",
    "    '''\n",
    "    util class to make a socrata client from a .yaml config file\n",
    "    '''\n",
    "    def __init__(self, inputdir, configItems):\n",
    "        self.inputdir = inputdir\n",
    "        self.configItems = configItems\n",
    "        \n",
    "    def connectToSocrata(self):\n",
    "        clientConfigFile = self.inputdir + self.configItems['socrata_client_config_fname']\n",
    "        with open(clientConfigFile,  'r') as stream:\n",
    "            try:\n",
    "                client_items = yaml.load(stream)\n",
    "                client = Socrata(client_items['url'],  client_items['app_token'], username=client_items['username'], password=base64.b64decode(client_items['password']))\n",
    "                return client\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "        return 0\n",
    "\n",
    "class pyLogger:\n",
    "    def __init__(self, configItems):\n",
    "        self.logfn = configItems['exception_logfile']\n",
    "        self.log_dir = configItems['log_dir']\n",
    "        self.logfile_fullpath = self.log_dir+self.logfn\n",
    "\n",
    "    def setConfig(self):\n",
    "        #open a file to clear log\n",
    "        fo = open(self.logfn, \"w\")\n",
    "        fo.close\n",
    "        logging.basicConfig(level=logging.DEBUG, filename=self.logfn, format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "        logger=logging.getLogger(__name__)\n",
    "            #self.logfile_fullpath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class form700:\n",
    "    '''\n",
    "    class to grab form 700 data off the API\n",
    "    '''\n",
    "    def __init__(self, configItems):\n",
    "        self.username = configItems['form700_username']\n",
    "        self.password = configItems['form700_password']\n",
    "        self.authUrl = configItems['authUrl']\n",
    "        self.url_cover = configItems['url_cover']\n",
    "        self.url_schedule = configItems['url_schedule']\n",
    "        self.agency_prefix = configItems['agency_prefix']\n",
    "        self.page_size = 1000\n",
    "        self.headers = {'content-type': 'application/json'}\n",
    "        self.cookies = self.grabCookies()\n",
    "        self.schedules = ['scheduleA1', 'scheduleA2', 'scheduleB', 'scheduleC', 'scheduleD', 'scheduleE', 'comments']\n",
    "        self.schedules_redacted = ['scheduleA1_redacted', 'scheduleA2_redacted', 'scheduleB_redacted', 'scheduleC_redacted', 'scheduleD_redacted', 'scheduleE_redacted', 'comments_redacted']\n",
    "    \n",
    "    def getSchedules(self):\n",
    "        return self.schedules\n",
    "    \n",
    "    def getSchedulesRedacted(self):\n",
    "        return self.schedules_redacted\n",
    "\n",
    "    def grabCookies(self):\n",
    "        '''\n",
    "        gets session cookie; then sets cookie property in the object\n",
    "        '''\n",
    "        authUrl = self.authUrl + '?UserName=' + self.username + '&Password='+ base64.b64decode(self.password)\n",
    "        r = requests.post(authUrl)\n",
    "        return r.cookies\n",
    "    \n",
    "    def makeRequest(self, url, current_page, isRedacted):\n",
    "        '''\n",
    "        method to make API calls to form 700 API\n",
    "        '''\n",
    "        rContent = None\n",
    "        responseJson = None\n",
    "        payload = {'AgencyPrefix': self.agency_prefix, 'CurrentPageIndex': current_page, 'PageSize': self.page_size, 'IsRedacted': isRedacted}\n",
    "        rContent = requests.post(url, params=payload, headers = self.headers, cookies=self.cookies)\n",
    "        try:\n",
    "            responseJson =  json.loads(rContent.content)\n",
    "        except:\n",
    "            print \"could not load content as json\"\n",
    "        return responseJson\n",
    "    \n",
    "    def getJsonData(self, url, isRedacted, keyToPluck=None):\n",
    "        '''\n",
    "        method to grab json data from API response\n",
    "        '''\n",
    "        total_pages = 0\n",
    "        current_page = 0\n",
    "        filing_data = []\n",
    "        while current_page <= total_pages:\n",
    "            current_page = current_page + 1\n",
    "            responseJson = self.makeRequest(url, current_page, isRedacted)\n",
    "            if keyToPluck:\n",
    "                filing_data = filing_data + responseJson[keyToPluck]\n",
    "            else:\n",
    "                filing_data = filing_data + [responseJson]\n",
    "            total_pages = responseJson['totalMatchingPages']\n",
    "        return filing_data\n",
    "    \n",
    "    def getCoverData(self, isRedacted=False):\n",
    "        '''\n",
    "        returns cover sheet data\n",
    "        '''\n",
    "        cover_data = None\n",
    "        cover_data = self.getJsonData(self.url_cover, isRedacted, 'filings')\n",
    "        if cover_data:\n",
    "            cover_data = json_normalize(cover_data)\n",
    "        if isRedacted:\n",
    "            cover_data = {'cover_redacted':cover_data}\n",
    "        else:\n",
    "            cover_data = {'cover':cover_data}\n",
    "        return cover_data\n",
    "    \n",
    "    def getScheduleData(self, isRedacted=False):\n",
    "        '''\n",
    "        returns schedules A1, A2, B, C, D and E\n",
    "        '''\n",
    "        parsed_schedulesDict = {}\n",
    "        schedule_data = self.getJsonData(self.url_schedule, isRedacted)\n",
    "        parsed_schedules = [  self.parseScheduleData(schedule, schedule_data) for schedule in self.schedules ]\n",
    "        for i in range(len(self.schedules)):\n",
    "            parsed_schedulesDict[self.schedules[i]] = parsed_schedules[i][self.schedules[i]]\n",
    "        if isRedacted:\n",
    "            parsed_schedulesDict = { k+\"_redacted\":v for k,v in parsed_schedulesDict.iteritems()}\n",
    "        return parsed_schedulesDict\n",
    "    \n",
    "    @staticmethod\n",
    "    def parseScheduleData(schedule, schedule_data):\n",
    "        '''\n",
    "        normalizes json data into python pandas dataframe; saves it as a dict objec\n",
    "        '''\n",
    "        single_schedule_list = [ data[schedule] for data in schedule_data]\n",
    "        #chain the list together\n",
    "        single_schedule_list = list(itertools.chain(*single_schedule_list ))\n",
    "        return  {schedule: json_normalize( single_schedule_list) }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class prepareDataSetSchema:\n",
    "    '''\n",
    "    class to prepare csv files of the dataset schema\n",
    "    '''\n",
    "    def __init__(self, configItems=None):\n",
    "        self.schema_dir = configItems['schema_dir']\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_columns(df):\n",
    "        '''\n",
    "        makes column schemas\n",
    "        '''\n",
    "        schema = []\n",
    "        columnNames = list(df.columns)\n",
    "        names = [ inflection.titleize(k) for k in columnNames]\n",
    "        for i in range(len(columnNames)):\n",
    "            col = {}\n",
    "            col['fieldName'] = columnNames[i]\n",
    "            col['name'] = names[i]\n",
    "            col['dataTypeName'] = None\n",
    "            schema.append(col)\n",
    "        schema = pd.DataFrame(schema)\n",
    "        return schema\n",
    "    \n",
    "    def makeSchemaCsv(self, df, dfName):\n",
    "        '''\n",
    "        outputs schema into csv \n",
    "        '''\n",
    "        fname = self.schema_dir + \"/\" + dfName+ \".csv\"\n",
    "        schema = self.make_columns(df)\n",
    "        if(not(os.path.isfile(fname))): \n",
    "            schema.to_csv(fname)\n",
    "            return \"created\" + fname\n",
    "        else:\n",
    "            return fname + \"exists! No file created\"\n",
    "       \n",
    "    def makeSchemaSchedules(self,  schedule_data, schedules):\n",
    "        '''outputs schedules as a csvs'''\n",
    "        return [ self.makeSchemaCsv(schedule_data[schedule], \"form700_\"+ schedule+ \"_schema\") for schedule in schedules]\n",
    "    \n",
    "    def makeSchemaOutFiles(self, schedule_data,cover_data, schedules ):\n",
    "        '''\n",
    "        method to output all the csvs files- need to fill out the column data types\n",
    "        '''\n",
    "        schedule_schemas = self.makeSchemaSchedules(schedule_data, schedules)\n",
    "        cover_schema = self.makeSchemaCsv(cover_data, 'form700_cover_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class dataSetPrep:\n",
    "    '''\n",
    "    class to clean up dataframe objects and cast columns based on defined schema\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, configItems):\n",
    "        self.schema_dir = configItems['schema_dir']\n",
    "\n",
    "    \n",
    "    def cleanDataSet(self, df_dict, dfname, tables):\n",
    "        schema_fname = self.schema_dir + \"form700_\"+ dfname + \"_schema.csv\"\n",
    "        df = self.checkForListColumns(tables, df_dict, dfname)\n",
    "        fields = pd.read_csv(schema_fname)\n",
    "        fieldnames = list(fields['fieldName'])\n",
    "        dataTypeName = list(fields['dataTypeName'])\n",
    "        fieldTypesDict = dict(zip(fieldnames, dataTypeName))\n",
    "        df = df[fieldnames]\n",
    "        df = self.removeNewLines(df)\n",
    "        df = self.castFields(df, fieldTypesDict)\n",
    "        return df\n",
    "    \n",
    "  \n",
    "    def castFields(self, df, fieldTypesDict):\n",
    "        for field, ftype in fieldTypesDict.iteritems():\n",
    "            if ftype == 'number':\n",
    "                try:\n",
    "                    df[field]= df[field].astype(str)\n",
    "                except:\n",
    "                    df[field]= df.apply(lambda row: self.castAscii(row,field),axis=1)\n",
    "                \n",
    "                df[field] = df[field].replace({'[a-zA-Z%]': 0}, regex=True)\n",
    "                try: \n",
    "                    df[field] = df[field].fillna(value=0)\n",
    "                    df[field] = df[field].astype(int)\n",
    "                except:\n",
    "                    df[field] = df[field].fillna(value=0.0)\n",
    "                    df[field] = df[field].apply(pd.to_numeric, errors='coerce').fillna(value=0.0)\n",
    "            elif ftype == 'text':\n",
    "                df[field] = df[field].fillna(value=\"\")\n",
    "                try:\n",
    "                    df[field]= df[field].astype(str)\n",
    "                except:\n",
    "                    df[field]= df.apply(lambda row: self.castAscii(row,field),axis=1)\n",
    "            elif ftype == 'checkbox':\n",
    "                #df[field] = df[field].replace({'False': False}, regex=True)\n",
    "                #df[field] = df[field].replace({'True': True}, regex=True)\n",
    "                df[field] = df[field].fillna(value=False)\n",
    "                #df[field]= df[field].astype(bool)\n",
    "            #elif ftype == 'date':\n",
    "               # df[field] = df[field].fillna(value=\"\")\n",
    "                #df[field] = pd.to_datetime(df[field], errors='coerce' , format='%Y%m%d')\n",
    "                #df[field]= df[field].astype('datetime64[ns]')\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def castAscii(row, field):\n",
    "        item =  row[field].encode('ascii','backslashreplace')\n",
    "        return item\n",
    "    \n",
    "    @staticmethod\n",
    "    def removeNewLines(df):\n",
    "        return df.replace({'\\n': ''}, regex=True)\n",
    "    \n",
    "    def cleanDataSetDict(self, dataset_dict, tables):\n",
    "        for df_key,df_val in dataset_dict.iteritems():\n",
    "            cleaned_df = self.cleanDataSet( dataset_dict, df_key, tables)\n",
    "            dataset_dict[df_key] = cleaned_df\n",
    "        return dataset_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_json(row, field):\n",
    "        jsonField = row[field]\n",
    "        allItems = []\n",
    "        for item in jsonField:\n",
    "            item_kv = []\n",
    "            for k,v in item.iteritems():\n",
    "                try:\n",
    "                    if str(v) == \"\":\n",
    "                        v = None\n",
    "                    item_str = str(k) + \":\" + str(v)\n",
    "                except:\n",
    "                    item_str = k.encode('ascii','backslashreplace')\n",
    "                item_kv.append(item_str)\n",
    "            line_item =  \",\".join(item_kv)\n",
    "            allItems.append(line_item)\n",
    "        allItems = \"|\".join(allItems)\n",
    "        return allItems\n",
    "    \n",
    "    def checkForListColumns(self, tables, df_dict, dfname):\n",
    "        table = tables[tables['df_name'] == dfname]\n",
    "        list_columns = table[['list_columns']]\n",
    "        list_columns = list_columns.iloc[0]['list_columns']\n",
    "        df = df_dict[dfname]\n",
    "        if list_columns != 0:\n",
    "            list_columns = list_columns.split(\":\")\n",
    "            #df = df_dict[dfname]\n",
    "            for col in list_columns:\n",
    "                #print col\n",
    "                if(not(col == 'gifts' or col == 'realProperties')):\n",
    "                    df[col] = df.apply(lambda row: self.flatten_json(row, col), axis=1)\n",
    "            itemToExplode = False\n",
    "            if \"gifts\" in list_columns:\n",
    "                itemToExplode = 'gifts'\n",
    "            if \"realProperties\" in list_columns:\n",
    "                itemToExplode = 'realProperties'\n",
    "            if itemToExplode:\n",
    "                df = self.explodeGiftsAndProperties(df, itemToExplode)\n",
    "        return df\n",
    "    \n",
    "    def joinFilerToSchedule(self, schedule_data, cover_data_df):\n",
    "        filier_cols = ['filingId', 'filerName', 'departmentName', 'positionName', 'offices', 'periodStart', 'periodEnd', 'filingDate']\n",
    "        df_filer_info = cover_data_df[filier_cols]\n",
    "        schedules = schedule_data.keys()\n",
    "        for schedule in schedules:\n",
    "            schedule_data[schedule] = pd.merge(schedule_data[schedule],df_filer_info , on='filingId', how='left')\n",
    "        return schedule_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def explodeGiftsAndProperties(df, field):\n",
    "        def renameRealPropertyCols(df):\n",
    "            dfcolsOrig = list(df.columns)\n",
    "            dfcols = [  col[0].upper()+ col[1:]  for col in dfcolsOrig ]\n",
    "            dfcolPrefix = [\"realProperty\" + col for col in dfcols]\n",
    "            dfcolsFinal = dict(zip(dfcolsOrig,dfcolPrefix))\n",
    "            df=df.rename(columns = dfcolsFinal)\n",
    "            return df\n",
    "        def blowupGifts(row, field, idx):\n",
    "            newdf = None\n",
    "            newdfList = list(row[field])\n",
    "            if len(newdfList[0]) > 0:\n",
    "                newdf = json_normalize(newdfList[0])\n",
    "                if field == \"realProperties\":\n",
    "                    newdf = renameRealPropertyCols(newdf)\n",
    "                newdf['index_col'] = idx\n",
    "            return newdf\n",
    "        df['index_col'] = df.index\n",
    "        dfListItems = pd.DataFrame()\n",
    "        #print len(dfListItems)\n",
    "        rowsidx = list(df['index_col'])\n",
    "        for idx in rowsidx:\n",
    "            row = df[ df['index_col'] == idx]\n",
    "            rowdf = blowupGifts(row, field, idx)\n",
    "            dfListItems = dfListItems.append(rowdf)\n",
    "        df = df.merge(dfListItems, how='left', on='index_col')\n",
    "        del df['index_col']\n",
    "        del df[field]\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SocrataCreateInsertUpdateForm700Data:\n",
    "    '''\n",
    "    creates dataset on socrata or inserts it if table exists\n",
    "    '''\n",
    "    def __init__(self,  configItems, client=None):\n",
    "        self.client = client\n",
    "        self.schema_dir = configItems['schema_dir']\n",
    "        self.tables = self.setTableInfo()\n",
    "        self.dataset_base_url = configItems['dataset_base_url']\n",
    "        self.chunkSize = 1000\n",
    "    \n",
    "    def getTableInfo(self):\n",
    "        return self.tables\n",
    "        \n",
    "    def setTableInfo(self):\n",
    "        tables_fname = self.schema_dir + \"form700_tables.csv\"\n",
    "        return pd.read_csv(tables_fname)\n",
    "    \n",
    "    def makeDataSet(self, dfname):\n",
    "        dataset = {}\n",
    "        dataset['columns'] = self.getColumns(dfname)\n",
    "        dataset = self.parseTableInfo(dataset, dfname)\n",
    "        return dataset\n",
    "    \n",
    "    def getColumns(self, dfname):\n",
    "        '''\n",
    "        creates columns as defined in schema csvs\n",
    "        '''\n",
    "        schema_fname = self.schema_dir + \"form700_\"+ dfname + \"_schema.csv\"\n",
    "        fields = pd.read_csv(schema_fname)\n",
    "        fieldnames = list(fields['fieldName'])\n",
    "        fieldnames =  [ field.replace(\".\", \"\") for field in fieldnames]\n",
    "        #note: this is changing shit into plural \n",
    "        fieldnames = [ inflection.underscore(field) for field in fieldnames]\n",
    "        names = list(fields['name'])\n",
    "        dataTypeName = list(fields['dataTypeName'])\n",
    "        columns = zip(fieldnames, names, dataTypeName)\n",
    "        columns = [{\"fieldName\":item[0], \"name\":item[1], \"dataTypeName\": item[2]} for item in columns ]\n",
    "        return columns\n",
    "    \n",
    "    def parseTableInfo(self, socrata_dataset, dfname):\n",
    "        self.tables['FourByFour']= self.tables['FourByFour'].fillna(0)\n",
    "        table = self.tables[self.tables['df_name'] == dfname].iloc[0]  #just want the first row\n",
    "        socrata_dataset['description'] = table['description']\n",
    "        socrata_dataset['tags'] = [table['tags']]\n",
    "        socrata_dataset['category'] = table['category']\n",
    "        socrata_dataset['dataset_name'] = table['dataset_name']\n",
    "        socrata_dataset['FourByFour'] = table['FourByFour']\n",
    "        socrata_dataset['redacted'] = table['redacted']\n",
    "        return socrata_dataset\n",
    "    \n",
    "    def createDataSet(self, dfname):\n",
    "        dataset = self.makeDataSet(dfname)\n",
    "        if dataset['FourByFour']== 0:\n",
    "            try:\n",
    "                socrata_dataset = self.client.create(dataset['dataset_name'], description=dataset['description'], columns=dataset['columns'], category=dataset['category'], tags=dataset['tags'], new_backend=False)\n",
    "                FourXFour = str(socrata_dataset['id'])\n",
    "                dataset['Dataset URL'] = self.dataset_base_url + FourXFour\n",
    "                dataset['FourByFour'] = FourXFour\n",
    "                print \"4x4 \"+dataset['FourByFour']\n",
    "            except:\n",
    "                print \"*****ERROR*******\"\n",
    "                dataset['Dataset URL'] = ''\n",
    "                dataset['FourByFour'] = 'Error: did not create dataset'\n",
    "                print \"4x4 \"+ dataset['FourByFour']\n",
    "                print \"***************\"\n",
    "        return dataset\n",
    "    \n",
    "    def insertDataSet(self, dataset, dataset_dict, dfname):\n",
    "        insertDataSet = []\n",
    "        rejectedChunks = []\n",
    "        #keep track of the rows we are inserting\n",
    "        dataset['rowsInserted'] = 0\n",
    "        dataset['totalRecords'] = 0\n",
    "        print \"dataset: \" + dfname + \": \" + dataset['FourByFour']\n",
    "        ##need to rename all the columns to fit socrata- need to use titlize\n",
    "        fieldnames = list(dataset_dict[dfname].columns)\n",
    "        fieldnamesRemoveDots =  [ field.replace(\".\", \"\") for field in fieldnames]\n",
    "        df_fields = [ inflection.underscore(field) for field in fieldnamesRemoveDots]\n",
    "        columndict = dict(zip(fieldnames,df_fields ))\n",
    "        dataset_dict[dfname] = dataset_dict[dfname].rename(columns=columndict)\n",
    "        #fill in all NAs just to be safe\n",
    "        dataset_dict[dfname] = dataset_dict[dfname].fillna(\"\")\n",
    "        \n",
    "        try:\n",
    "            insertDataSet = dataset_dict[dfname].to_dict('records')\n",
    "            dataset['totalRecords'] = len(insertDataSet)\n",
    "        except:\n",
    "            print 'Error: could not get data'\n",
    "            return dataset\n",
    "        \n",
    "        insertChunks = self.makeChunks(insertDataSet)\n",
    "        #overwrite the dataset on the first insert chunk[0]\n",
    "        if dataset['rowsInserted'] == 0:\n",
    "            rejectedChunk = self.replaceDataSet(dataset, insertChunks[0])\n",
    "            if len(insertChunks) > 1:\n",
    "                for chunk in insertChunks[1:]:\n",
    "                    rejectedChunk = self.insertData(dataset, chunk)\n",
    "        else:\n",
    "            for chunk in insertChunk:\n",
    "                rejectedChunk = self.insertData(dataset, chunk)\n",
    "        return dataset\n",
    "    \n",
    "    @retry( tries=10, delay=1, backoff=2)\n",
    "    def replaceDataSet(self, dataset, chunk):\n",
    "        result = self.client.replace( dataset['FourByFour'], chunk ) \n",
    "        dataset['rowsInserted'] = dataset['rowsInserted'] + int(result['Rows Created'])\n",
    "        time.sleep(0.25)\n",
    "        \n",
    "        \n",
    "    @retry( tries=10, delay=1, backoff=2)\n",
    "    def insertData(self, dataset, chunk):\n",
    "        result = self.client.upsert(dataset['FourByFour'], chunk) \n",
    "        dataset['rowsInserted'] = dataset['rowsInserted'] + int(result['Rows Created'])\n",
    "        time.sleep(0.25)\n",
    "       \n",
    "\n",
    "    def makeChunks(self, insertDataSet):\n",
    "        return [insertDataSet[x:x+ self.chunkSize] for x in xrange(0, len(insertDataSet), self.chunkSize)]\n",
    "    \n",
    "    \n",
    "    def postDataToSocrata(self, dfname, dataset_dict ):\n",
    "        dataset = self.createDataSet(dfname)\n",
    "        if dataset['FourByFour']!= 0:\n",
    "            dataset = self.insertDataSet( dataset, dataset_dict, dfname)\n",
    "        else: \n",
    "            print \"dataset does not exist\"\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class emailer():\n",
    "    '''\n",
    "    util class to email stuff to people.\n",
    "    '''\n",
    "    def __init__(self, inputdir, configItems):\n",
    "        self.inputdir = inputdir\n",
    "        self.configItems = configItems\n",
    "        self.emailConfigs = self.getEmailerConfigs()\n",
    "        \n",
    "        \n",
    "    def getEmailerConfigs(self):\n",
    "        emailConfigFile = self.inputdir + self.configItems['email_config_fname']\n",
    "        with open(emailConfigFile,  'r') as stream:\n",
    "            try:\n",
    "                email_items = yaml.load(stream)\n",
    "                return email_items\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "        return 0\n",
    "    \n",
    "    def setConfigs(self, subject_line, msgBody, fname_attachment=None, fname_attachment_fullpath=None):\n",
    "        self.server = self.emailConfigs['server_addr']\n",
    "        self.server_port = self.emailConfigs['server_port']\n",
    "        self.address =  self.emailConfigs['email_addr']\n",
    "        self.password = base64.b64decode(self.emailConfigs['email_pass'])\n",
    "        self.msgBody = msgBody\n",
    "        self.subjectLine = subject_line\n",
    "        self.fname_attachment = fname_attachment\n",
    "        self.fname_attachment_fullpath = fname_attachment_fullpath\n",
    "        self.recipients = self.emailConfigs['receipients']\n",
    "        self.recipients =  self.recipients.split(\",\")\n",
    "    \n",
    "    def getEmailConfigs(self):\n",
    "        return self.emailConfigs\n",
    "    \n",
    "    def sendEmails(self, subject_line, msgBody, fname_attachment=None, fname_attachment_fullpath=None):\n",
    "        self.setConfigs(subject_line, msgBody, fname_attachment, fname_attachment_fullpath)\n",
    "        fromaddr = self.address\n",
    "        toaddr = self.recipients\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = fromaddr\n",
    "        msg['To'] = \", \".join(toaddr)\n",
    "        msg['Subject'] = self.subjectLine\n",
    "        body = self.msgBody \n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "          \n",
    "        #Optional Email Attachment:\n",
    "        if(not(self.fname_attachment is None and self.fname_attachment_fullpath is None)):\n",
    "            filename = self.fname_attachment\n",
    "            attachment = open(self.fname_attachment_fullpath, \"rb\")\n",
    "            part = MIMEBase('application', 'octet-stream')\n",
    "            part.set_payload((attachment).read())\n",
    "            encoders.encode_base64(part)\n",
    "            part.add_header('Content-Disposition', \"attachment; filename= %s\" % filename)\n",
    "            msg.attach(part)\n",
    "        \n",
    "        #normal emails, no attachment\n",
    "        server = smtplib.SMTP(self.server, self.server_port)\n",
    "        server.starttls()\n",
    "        server.login(fromaddr, self.password)\n",
    "        text = msg.as_string()\n",
    "        server.sendmail(fromaddr, toaddr, text)\n",
    "        server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class logETLLoad:\n",
    "    '''\n",
    "    util class to get job status- aka check to make sure that records were inserted; also emails results to receipients\n",
    "    '''\n",
    "    def __init__(self, inputdir, configItems):\n",
    "        self.keysToRemove = ['columns', 'tags']\n",
    "        self.log_dir = configItems['log_dir']\n",
    "        self.dataset_base_url = configItems['dataset_base_url']\n",
    "        self.failure =  False\n",
    "        self.job_name = configItems['job_name']\n",
    "        self.logfile_fname = self.job_name + \".csv\"\n",
    "        self.logfile_fullpath = self.log_dir + self.job_name + \".csv\"\n",
    "        self.configItems =  configItems\n",
    "        self.inputdir = inputdir\n",
    "        \n",
    "    def removeKeys(self, dataset):\n",
    "        for key in self.keysToRemove:\n",
    "            try:\n",
    "                remove_columns = dataset.pop(key, None)\n",
    "            except:\n",
    "                noKey = True\n",
    "        return dataset\n",
    "    \n",
    "    def sucessStatus(self, dataset):\n",
    "        dataset = self.removeKeys(dataset)\n",
    "        if dataset['rowsInserted'] == dataset['totalRecords']:\n",
    "            dataset['jobStatus'] = \"SUCCESS\"\n",
    "        else: \n",
    "            dataset['jobStatus'] = \"FAILURE\"\n",
    "            self.failure =  True\n",
    "        return dataset\n",
    "    \n",
    "    def makeJobStatusAttachment(self,  finishedDataSets ):\n",
    "        with open(self.logfile_fullpath, 'w') as csvfile:\n",
    "            fieldnames = finishedDataSets[0].keys()\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for dataset in finishedDataSets:\n",
    "                writer.writerow(dataset)\n",
    "\n",
    "    def getJobStatus(self):\n",
    "        if self.failure: \n",
    "            return  \"FAILED: \" + self.job_name\n",
    "        else: \n",
    "            return  \"SUCCESS: \" + self.job_name\n",
    "\n",
    "    def makeJobStatusMsg( self,  dataset  ):\n",
    "        msg = dataset['jobStatus'] + \": \" + dataset['dataset_name'] + \"-> Total Rows:\" + str(dataset['totalRecords']) + \", Rows Inserted: \" + str(dataset['rowsInserted'])  + \", Link: \"  + self.dataset_base_url + dataset['FourByFour'] + \" \\n\\n \" \n",
    "        return msg\n",
    "    \n",
    "    def sendJobStatusEmail(self, finishedDataSets):\n",
    "        msgBody  = \"\" \n",
    "        for i in range(len(finishedDataSets)):\n",
    "            #remove the column definitions, check if records where inserted\n",
    "            dataset = self.sucessStatus( self.removeKeys(finishedDataSets[i]))\n",
    "            msg = self.makeJobStatusMsg( dataset  )\n",
    "            msgBody  = msgBody  + msg\n",
    "        subject_line = self.getJobStatus()\n",
    "        email_attachment = self.makeJobStatusAttachment(finishedDataSets)\n",
    "        e = emailer(self.inputdir, self.configItems)\n",
    "        emailconfigs = e.getEmailConfigs()\n",
    "        if os.path.isfile(self.logfile_fullpath):\n",
    "            e.sendEmails( subject_line, msgBody, self.logfile_fname, self.logfile_fullpath)\n",
    "        else:\n",
    "            e.sendEmails( subject_line, msgBody)\n",
    "        print \"****************JOB STATUS******************\"\n",
    "        print subject_line\n",
    "        print \"Email Sent!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#needed to create schema csv files \n",
    "#cds= createDataSets(config_dir, configItems)\n",
    "#schemas = cds.makeSchemaOutFiles(schedule_data,cover_data, schedules )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDataAndUpload(finishedDataSets, isRedacted=False):\n",
    "    #get the data\n",
    "    cover_data = f700.getCoverData(isRedacted)\n",
    "    schedule_data = f700.getScheduleData(isRedacted) \n",
    "    #join cover data to schedules \n",
    "    if isRedacted:\n",
    "        cover_key = 'cover_redacted'\n",
    "    else:\n",
    "        cover_key = 'cover'\n",
    "    schedule_data = dsp.joinFilerToSchedule(schedule_data, cover_data[cover_key])\n",
    "    #clean the cover data + schedule datasets \n",
    "    cover_data[cover_key] = dsp.cleanDataSet(cover_data,  cover_key, tables)\n",
    "    schedule_data = dsp.cleanDataSetDict(schedule_data, tables)\n",
    "    schedule_list = schedule_data.keys()\n",
    "    #post the redacted data\n",
    "    dataset = scICU.postDataToSocrata(cover_key, cover_data)\n",
    "    finishedDataSets.append(dataset)\n",
    "    for schedule in schedule_list: \n",
    "        dataset = scICU.postDataToSocrata(schedule, schedule_data)\n",
    "        finishedDataSets.append(dataset)\n",
    "    return finishedDataSets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputdir = \"/home/ubuntu/workspace/configFiles/\"\n",
    "fieldConfigFile = 'fieldConfig.yaml'\n",
    "cI =  ConfigItems(inputdir ,fieldConfigFile  )\n",
    "configItems = cI.getConfigs()\n",
    "sc = SocrataClient(inputdir, configItems)\n",
    "client = sc.connectToSocrata()\n",
    "#class objects\n",
    "scICU = SocrataCreateInsertUpdateForm700Data(configItems,client)\n",
    "dsp = dataSetPrep(configItems)\n",
    "lte = logETLLoad(inputdir, configItems)\n",
    "f700 = form700(configItems)\n",
    "tables = scICU.getTableInfo()\n",
    "tables = tables.fillna(0)\n",
    "lg = pyLogger(configItems)\n",
    "lg.setConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputting Form 700 Datasets\n",
      "2016-06-09 19:36:55.791856\n",
      "\n",
      "cover vjxe-a4ce"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 1 seconds...\n",
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 2 seconds...\n",
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 4 seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scheduleD aez9-nje2\n",
      "scheduleE 9cv6-hatq\n",
      "scheduleB fcif-wqha\n",
      "scheduleC ff9q-5i8s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 1 seconds...\n",
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 2 seconds...\n",
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 4 seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "comments kxf2-hbth\n",
      "scheduleA2 bih6-5p6g\n",
      "scheduleA1 vs94-xyyc\n",
      "cover_redacted ni6u-knhb\n",
      "scheduleE_redacted 2ige-b5yn"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 1 seconds...\n",
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 2 seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scheduleC_redacted fkhv-84jp\n",
      "scheduleA2_redacted j82c-uj4d\n",
      "scheduleD_redacted kpf8-y8tj\n",
      "scheduleB_redacted tcn4-z9dy\n",
      "scheduleA1_redacted tzbk-2xg8\n",
      "comments_redacted e9vv-n9ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 1 seconds...\n",
      "WARNING:retry.api:409 Client Error: Conflict.\n",
      "\tAnother process is writing to the dataset; please try again later, retrying in 2 seconds...\n",
      "WARNING:retry.api:HTTPSConnectionPool(host='data.sfgov.org', port=443): Read timed out. (read timeout=10), retrying in 4 seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************JOB STATUS******************\n",
      "SUCCESS: Form 700 ETL\n",
      "Email Sent!\n"
     ]
    }
   ],
   "source": [
    "print \"Outputting Form 700 Datasets\"\n",
    "print datetime.datetime.now()\n",
    "print \n",
    "finishedDataSets  = []\n",
    "#get the private datasets\n",
    "finishedDataSets = getDataAndUpload(finishedDataSets, False)\n",
    "#get the redacted datasets\n",
    "finishedDataSets = getDataAndUpload(finishedDataSets, True)\n",
    "msg  = lte.sendJobStatusEmail(finishedDataSets)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if __name__ == '__main__' and '__file__' in globals():\n",
    "  #  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
